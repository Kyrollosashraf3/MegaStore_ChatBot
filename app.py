import sys
!{sys.executable} -m pip install langchain-community==0.2.12


import streamlit as st
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from transformers import pipeline

# -------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Streamlit
# -------------------------------
st.set_page_config(page_title="ğŸ›ï¸ MegaStore AI Assistant", page_icon="ğŸ›’", layout="centered")
st.title("ğŸ›ï¸ MegaStore AI Assistant")
st.write("Welcome! Chat with MegaStoreâ€™s AI to learn more about our products and services.")

# -------------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¯Ø§ØªØ§ Ù…Ù† Ø§Ù„Ù…Ù„Ù
# -------------------------------
@st.cache_data
def load_data():
    with open("megastore_dataset.txt", "r", encoding="utf-8") as f:
        return f.readlines()

data = load_data()

# -------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Embeddings Ùˆ Vector DB
# -------------------------------
@st.cache_resource
def create_vector_db():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return FAISS.from_texts(data, embeddings)

vector_db = create_vector_db()

# -------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ LLM Ùˆ Ø§Ù„Ù€ Chain
# -------------------------------
@st.cache_resource
def create_conversational_chain():
    qa_pipeline = pipeline(
        "text2text-generation",
        model="google/flan-t5-base",
        max_new_tokens=256
    )
    llm = HuggingFacePipeline(pipeline=qa_pipeline)
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_db.as_retriever(),
        memory=memory
    )
    return chain

qa = create_conversational_chain()

# -------------------------------
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
# -------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
if question := st.chat_input("Type your question here..."):
    # Ø£Ø¶Ù Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    # Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            result = qa({"question": question})
            answer = result["answer"]
            st.markdown(answer)

    # Ø§Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø©
    st.session_state.messages.append({"role": "assistant", "content": answer})
