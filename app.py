# -------------------------------
# ğŸ›ï¸ MegaStore AI Assistant (Stable Streamlit Version - Fixed)
# -------------------------------

import streamlit as st
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter

# -------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Streamlit
# -------------------------------
st.set_page_config(page_title="ğŸ›ï¸ MegaStore AI Assistant", page_icon="ğŸ›’", layout="centered")
st.title("ğŸ›ï¸ MegaStore AI Assistant")
st.write("Welcome! Chat with MegaStoreâ€™s AI to learn more about our products and services.")

# -------------------------------
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¨Ù†Ø§Ø¡ Ø§Ù„Ø³Ù„Ø³Ù„Ø©
# -------------------------------
@st.cache_resource
def load_chain():
    try:
        file_path = "megastore_dataset.txt"

        with open(file_path, "r", encoding="utf-8") as f:
            data = f.read()

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,
            chunk_overlap=50,
            separators=["\n\n", "\n", ".", "!", "?", ",", " "]
        )
        chunks = splitter.split_text(data)

        # Ø¨Ù†Ø§Ø¡ embeddings
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        # Ø¨Ù†Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠØ©
        vector_db = FAISS.from_texts(chunks, embeddings)
        retriever = vector_db.as_retriever(search_kwargs={"k": 3})

        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        qa_pipeline = pipeline(
            "text2text-generation",
            model="google/flan-t5-base",
            max_new_tokens=100,
            temperature=0.2,
            device=-1
        )
        llm = HuggingFacePipeline(pipeline=qa_pipeline)

        # Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

        # Ø¨Ù†Ø§Ø¡ Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            verbose=False
        )

        return qa_chain
    except Exception as e:
        st.error(f"âš ï¸ Error while loading chain: {e}")
        return None


qa = load_chain()

# -------------------------------
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£Ø¬ÙˆØ¨Ø©)
# -------------------------------
if "messages" not in st.session_state:
    st.session_state["messages"] = []

user_input = st.text_input("Your Question:", placeholder="e.g. What services does MegaStore provide?")

if st.button("Ask") and user_input:
    with st.spinner("Thinking..."):
        if qa is None:
            answer_text = "âš ï¸ Model failed to load. Please check the logs."
        else:
            try:
                answer_text = qa.run(user_input)
            except Exception as e:
                answer_text = f"âš ï¸ Error: {e}"

        st.session_state["messages"].append((user_input, answer_text))

# Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
for question, answer in st.session_state["messages"]:
    st.markdown(f"**ğŸ§â€â™‚ï¸ You:** {question}")
    st.markdown(f"**ğŸ¤– Bot:** {answer}")
